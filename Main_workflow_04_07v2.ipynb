{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pansyhb/BigData/blob/main/Main_workflow_04_07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stock Predicitions with Sentiment Analysis using Big Data Tools"
      ],
      "metadata": {
        "id": "cOM7WMMLHLb3"
      },
      "id": "cOM7WMMLHLb3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.0 Introduction\n",
        "\n",
        "This project is a result of the collaboration between Adam Green and Pansy Ahmed for Data 6300 Big Data Analysis course project- Winter, 2023.\n",
        "\n",
        "This project aims to explore the accuracy of stock market predicitions in relation to Sentiment Analysis scraped from news headlines. \n",
        "\n",
        "It explores the stocks' news headline and financial parameters throughout 2018 -Feb to 2023-Feb.\n",
        "\n",
        "There are 3 main phases that go into the processing of data: **Data Collection** through streaming and web scraping, **Data storage** in Postgres and processing through Spark, **Training** sentiment data on an LSTM model, training the stock data on a Prophet model, and eventually creating a hybrid model and  examining the accuracy of that model on:\n",
        "\n",
        "- A single stock that was used for training - MSFT( Microsoft)\n",
        "- A single stock that was unseen - GOOG (Google)\n",
        "- A group of stocks (TSLA, GOOG, MSFT, AMZN, )\n",
        "\n",
        "# We have use Postgres and Spark to account for the scalability factor needed to manage big data. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RUlbEWFnHZhf"
      },
      "id": "RUlbEWFnHZhf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.0 Data Collection"
      ],
      "metadata": {
        "id": "8XwywoQNKj5P"
      },
      "id": "8XwywoQNKj5P"
    },
    {
      "cell_type": "code",
      "source": [
        "#install necessary libraries & software\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq>/dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz\n",
        "%pip install --upgrade VaderSentiment\n",
        "%pip install --upgrade -q findspark\n",
        "%pip install --upgrade -q pyspark.pandas\n",
        "#%pip install -U pyarrow\n",
        "!git clone https://github.com/Pansyhb/BigData.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZcgwEzQLmnf",
        "outputId": "ba739432-71d7-4dab-c6a3-ac5b36d04770"
      },
      "id": "wZcgwEzQLmnf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: VaderSentiment in /usr/local/lib/python3.9/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from VaderSentiment) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->VaderSentiment) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->VaderSentiment) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->VaderSentiment) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->VaderSentiment) (3.4)\n",
            "fatal: destination path 'BigData' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /usr/lib/jvm/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2zSYB9wlUEw",
        "outputId": "67ad7b92-7890-4285-e77c-6a1fa2599f7e"
      },
      "id": "V2zSYB9wlUEw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "java-1.11.0-openjdk-amd64  java-1.8.0-openjdk-amd64\n",
            "java-11-openjdk-amd64\t   java-8-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['JAVA_HOME']=\"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"]= \"/content/spark-3.3.2-bin-hadoop3\"\n"
      ],
      "metadata": {
        "id": "pPQVhMmQQ19_"
      },
      "id": "pPQVhMmQQ19_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libraries \n",
        "import pandas as pd\n",
        "import findspark\n",
        "import csv\n",
        "import datetime\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests"
      ],
      "metadata": {
        "id": "5FkUNMzio2vp"
      },
      "id": "5FkUNMzio2vp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "WS2kLwIUoWMF"
      },
      "id": "WS2kLwIUoWMF",
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initiate a spark instance\n",
        "#import pyspark\n",
        "findspark.init()\n",
        "from pyspark.sql.functions import sum,max,min,mean,count\n",
        "import datetime as dt\n",
        "from pyspark.sql import SparkSession\n",
        "import yaml\n",
        "from yaml.loader import SafeLoader\n",
        "from os.path import abspath\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O6s1vPJVHLJi"
      },
      "id": "O6s1vPJVHLJi",
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "conf = SparkConf().setAppName(\"test\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)"
      ],
      "metadata": {
        "id": "ORV0ZrMQLbJB"
      },
      "id": "ORV0ZrMQLbJB",
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "9705971d",
      "metadata": {
        "id": "9705971d"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "warehouse_location = abspath('spark-warehouse')\n",
        "with open('/content/BigData/Final_project_files/cfg2.yml') as f:\n",
        "    config = yaml.load(f, Loader = SafeLoader)\n",
        "\n",
        "    #create spark connection\n",
        "findspark.init()\n",
        "spark = SparkSession.builder \\\n",
        "    .master(config['spark']['spark_master'])\\\n",
        "    .appName('retrieve')\\\n",
        "    .enableHiveSupport()\\\n",
        "    .config('spark.sql.warehouse.dir', warehouse_location)\\\n",
        "    .config(config['spark']['spark_jars'], config['spark']['spark_jars_path'])\\\n",
        "    .config(\"spark.executor.extraClassPath\", config['spark']['spark_jars_path'])\\\n",
        "    .config('spark.cores.max', '2')\\\n",
        "    .config('spark.executor.cores', '2')\\\n",
        "    .getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "spark\n",
        "\n",
        "#create database config details\n",
        "url = config['postgres']['url']\n",
        "properties = {\n",
        "    'user': config['postgres']['user'],\n",
        "    'password' : config['postgres']['password'],\n",
        "    'url': config['postgres']['url'],\n",
        "    'driver': config['postgres']['driver']\n",
        "    \n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c295390",
      "metadata": {
        "id": "4c295390"
      },
      "source": [
        "# Retrieve Data from Database and Write to Csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "def return_data(self,ticker_list, from_date, to_date):\n",
        "    sentiment = spark.read.format(\"jdbc\")\\\n",
        "        .option(\"url\", \"jdbc:postgresql://localhost:5432/financials\") \\\n",
        "        .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"sentiment\") \\\n",
        "        .option(\"user\", \"adam\").option(\"password\", \"green\").load()\n",
        "    finance = spark.read.format(\"jdbc\")\\\n",
        "        .option(\"url\", \"jdbc:postgresql://localhost:5432/financials\") \\\n",
        "        .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"company_data\") \\\n",
        "        .option(\"user\", \"adam\").option(\"password\", \"green\").load()\n",
        "    full_data = finance.join(sentiment, ['date', 'ticker'], 'leftouter').fillna(0)\n",
        "    df_list = []"
      ],
      "metadata": {
        "id": "Ycf88UepRlNM"
      },
      "id": "Ycf88UepRlNM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentiment = spark.read\\\n",
        "      .format(\"jdbc\")\\\n",
        "      .option(\"driver\", properties['driver'])\\\n",
        "      .option(\"url\", url) \\\n",
        "      .option(\"dbtable\", \"sentiment.sentiment\") \\\n",
        "      .option(\"user\", properties['user']).option(\"password\", properties['password']).load()\n",
        "    finance = spark.read.format(\"jdbc\")\\\n",
        "      .option(\"url\", url) \\\n",
        "      .option(\"driver\", properties['driver']).option(\"dbtable\", \"company_data\") \\\n",
        "      .option(\"user\", properties['user']).option(\"password\", properties['password']).load()"
      ],
      "metadata": {
        "id": "ypEY8NDb1sFp"
      },
      "id": "ypEY8NDb1sFp"
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "228f7615",
      "metadata": {
        "id": "228f7615",
        "outputId": "859ed850-41d9-4e73-c2c8-7b63daef5877",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-b2a45274628e>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mdf_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mreturn_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MSFT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GOOG'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"2016-01-01\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"2023-03-01\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-87-b2a45274628e>\u001b[0m in \u001b[0;36mreturn_data\u001b[0;34m(ticker_list, from_date, to_date)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#retrieve data from database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreturn_data\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mticker_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jdbc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.2-bin-hadoop3/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     def json(\n",
            "\u001b[0;32m/content/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o348.load.\n: java.sql.SQLException: No suitable driver\n\tat java.sql.DriverManager.getDriver(DriverManager.java:315)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:107)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:107)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
          ]
        }
      ],
      "source": [
        "import pyspark.pandas as ps\n",
        "import psycopg2\n",
        "conn_string = \"postgres://postgres:postgrespw@localhost:32768\"\n",
        "#retrieve data from database\n",
        "def return_data( ticker_list, from_date, to_date):\n",
        "    sentiment = spark.read\\\n",
        "    .format(\"jdbc\")\\\n",
        "    .option(\"url\", url) \\\n",
        "    .option(\"dbtable\", \"sentiment\") \\\n",
        "    .option(\"user\", properties['user']).option(\"password\", properties['password']).load()\n",
        "   \n",
        "    finance = spark.read.format(\"jdbc\")\\\n",
        "    .option(\"url\", url) \\\n",
        "    .option(\"driver\", properties['driver']).option(\"dbtable\", \"company_data\") \\\n",
        "    .option(\"user\", properties['user']).option(\"password\", properties['password']).load()\n",
        "  \n",
        "    full_data = finance.join(sentiment, ['date', 'ticker'], 'leftouter').fillna(0)\n",
        "    print(sentiment.shape)\n",
        "    df_list = []\n",
        "\n",
        "    for ticker in ticker_list:\n",
        "      working_data = full_data[full_data['ticker'] == ticker]\n",
        "      print(working_data.count())\n",
        "      working_data = working_data.sort('date', ascending = True).filter((working_data.date >= from_date) & (working_data.date<= to_date)).toPandas().set_index('date')\n",
        "      #sorted_data = working_data.toPandas().loc[from_date:to_date,]\n",
        "      print(ticker, working_data.shape)\n",
        "      working_data = working_data[~working_data.index.duplicated()]\n",
        "      print(working_data)\n",
        "      working_data.to_csv('data/'+ticker+'_dataframe.csv')\n",
        "      df_list.append(working_data)\n",
        "\n",
        "return_data(['MSFT', 'GOOG'], \"2016-01-01\", \"2023-03-01\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41d4d8b0",
      "metadata": {
        "id": "41d4d8b0"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
