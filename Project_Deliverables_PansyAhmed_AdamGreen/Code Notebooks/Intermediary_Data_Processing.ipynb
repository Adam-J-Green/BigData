{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94fc6742",
   "metadata": {},
   "source": [
    "## Intermediary Data Processing\n",
    "This file is included as a necessary intermediary step to bridge the connection between data retrieval and model building due to technological barriers in accessing cloud software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9705971d",
   "metadata": {
    "id": "9705971d",
    "outputId": "26fe27b6-c0fa-4d9d-86de-3aa512cf8fff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/10 14:54:53 WARN Utils: Your hostname, cis6180 resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/04/10 14:54:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/04/10 14:54:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from pyspark.sql.functions import sum,max,min,mean,count\n",
    "import datetime as dt\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "from os.path import abspath\n",
    "\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "with open('cfg.yml') as f:\n",
    "    config = yaml.load(f, Loader = SafeLoader)\n",
    "\n",
    "    #create spark connection\n",
    "findspark.init()\n",
    "spark = SparkSession.builder \\\n",
    "    .master(config['spark']['spark_master'])\\\n",
    "    .appName('retrieve')\\\n",
    "    .enableHiveSupport()\\\n",
    "    .config('spark.sql.warehouse.dir', warehouse_location)\\\n",
    "    .config(config['spark']['spark_jars'], config['spark']['spark_jars_path'])\\\n",
    "    .config('spark.cores.max', '2')\\\n",
    "    .config('spark.executor.cores', '2')\\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "spark\n",
    "\n",
    "#create database config details\n",
    "url = config['postgres']['url']\n",
    "properties = {\n",
    "    'user': config['postgres']['user'],\n",
    "    'password' : config['postgres']['user'],\n",
    "    'url': url,\n",
    "    'driver': config['postgres']['driver']\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c295390",
   "metadata": {
    "id": "4c295390"
   },
   "source": [
    "# Retrieve Data from Database and Write to Csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "228f7615",
   "metadata": {
    "id": "228f7615",
    "outputId": "6af5c49d-d6c5-422c-bc1a-bef62951e26d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "#retrieve data from database\n",
    "def return_data(ticker_list, from_date, to_date):\n",
    "    sentiment = spark.read.format(\"jdbc\")\\\n",
    "        .option(\"url\", \"jdbc:postgresql://localhost:5432/financials\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"sentiment\") \\\n",
    "        .option(\"user\", \"adam\").option(\"password\", \"green\").load()\n",
    "    finance = spark.read.format(\"jdbc\")\\\n",
    "        .option(\"url\", \"jdbc:postgresql://localhost:5432/financials\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"company_data\") \\\n",
    "        .option(\"user\", \"adam\").option(\"password\", \"green\").load()\n",
    "    full_data = finance.join(sentiment, ['date', 'ticker'], 'leftouter').fillna(0)\n",
    "    df_list = []\n",
    "    for ticker in ticker_list:\n",
    "        working_data = full_data[full_data['ticker'] == ticker]\n",
    "        working_data = working_data.sort('date', ascending = True).filter((working_data.date >= from_date) & (working_data.date<= to_date)).toPandas().set_index('date')\n",
    "        working_data = working_data[~working_data.index.duplicated()]\n",
    "        working_data.to_csv('data/'+ticker+'_dataframe.csv')\n",
    "        df_list.append(working_data)\n",
    "return_data(['MSFT', 'GOOG', 'AMZN', 'TSLA', 'NFLX'], \"2016-01-01\", \"2023-03-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d4d8b0",
   "metadata": {
    "id": "41d4d8b0"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
